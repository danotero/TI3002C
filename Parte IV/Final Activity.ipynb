{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1cd3a8",
   "metadata": {},
   "source": [
    "# Final Activity\n",
    "\n",
    "In this activity you will improve the results obtained by the logistic regression model for predicting whether a person has diabetes. \n",
    "\n",
    "As you will see shortly, there are some lines that say `\"INSERT YOUR CODE HERE\"`, which in many cases means that you will literary replace that sentence by just one line of code, but in other cases you will need to write more lines of code so that your implementation works properly. Do what you need to do to make things work the way they should, but in general, your implementations should be done with a few lines. \n",
    "\n",
    "Now that we have that out of the way, let us load the libraries you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb013515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas                  as pd\n",
    "import numpy                   as np\n",
    "import matplotlib.pyplot       as plt\n",
    "import seaborn                 as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f51a1",
   "metadata": {},
   "source": [
    "To evaluate your model you will be using the following function. Please do not modify it. Keep in mind that it receives a dataframe with two columns: the `observed` column that contains the true labels of the test dataset, and the `prediction` column that holds the predictions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b782882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(results):\n",
    "    \n",
    "    positives = results[['observed', 'prediction']][results['observed'] == 1]\n",
    "    negatives = results[['observed', 'prediction']][results['observed'] == 0]\n",
    "    \n",
    "    true_negatives = negatives[negatives['observed'] == negatives['prediction']].shape[0]\n",
    "    false_positives = negatives[negatives['observed'] != negatives['prediction']].shape[0]\n",
    "    true_positives = positives[positives['observed'] == positives['prediction']].shape[0]\n",
    "    false_negatives = positives[positives['observed'] != positives['prediction']].shape[0]\n",
    "    \n",
    "    confusion_matrix = {'actual positives' : [true_positives, false_negatives], \n",
    "                        'actual negatives' : [false_positives, true_negatives]}\n",
    "    \n",
    "    confusion_matrix_df = pd.DataFrame.from_dict(confusion_matrix, orient='index', \n",
    "                                                 columns=['predicted positives', 'predicted negatives'])\n",
    "    \n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives +  true_negatives + false_negatives)\n",
    "    precission = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * (precission * recall) / (precission + recall)\n",
    "    \n",
    "    metrics = {'Accuracy' : accuracy, 'Precission' : precission, 'Recall' : recall, 'F1 Score' : f1_score}\n",
    "    \n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index', columns=['Metrics'])\n",
    "    \n",
    "    return confusion_matrix_df, metrics_df   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8619a6f8",
   "metadata": {},
   "source": [
    "You will be working, once again, with the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c57d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('diabetes-dataset.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66b0c7",
   "metadata": {},
   "source": [
    "It turns out that this dataset has a lot of repeated observations, so we will remove those extra rows using the `drop_duplicates` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b76b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5cd16",
   "metadata": {},
   "source": [
    "The original dataset had 2000 observations, and now we have 744, so there was a lot of redundancy in the data.\n",
    "\n",
    "Now we create the target variable $y$ and a dataframe $X$ in which we will store the values of all the observations of the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Outcome']\n",
    "X = data.drop(columns='Outcome')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06c6da",
   "metadata": {},
   "source": [
    "For models such as logistic regression and neural networks, it is good practice to standardize features by removing the mean and scaling to unit variance. That is, each feature $x_i$ is transformed as follows\n",
    "\n",
    "$$\\hat{x}_i=\\frac{x_i-\\bar{x}_i}{s_i},$$\n",
    "\n",
    "where $\\bar{x}_i$ and $s_i$ are the mean and the standard deviation of the values of the variable $x_i$, respectively. This tends to improve the convergence of the model during training. The standardization of features is implemented in the `StandardScaler` module of `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473fc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267d520",
   "metadata": {},
   "source": [
    "Now we are ready to create the **training** and **testing** datasets. For this task we have the rather famous `train_test_split` method from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c70d1b8",
   "metadata": {},
   "source": [
    "Note that for replicating results it is convenient to set the `random_state` parameter to an arbitrary number, this guarantees that each time you run this cell you will get the same training and test sets, but the way the elements are selected is random.\n",
    "\n",
    "At this point we have all we need to train the model. As expected, there is an implementation of logistic regression in `sklearn`. Train the model by running the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e591091",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(penalty=None).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1600d4",
   "metadata": {},
   "source": [
    "Create a variable called `y_pred` and store the predictions of the model for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b999a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = \"INSERT YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0adaf",
   "metadata": {},
   "source": [
    "For evaluation purposes, we will create a dataframe that contains both the true and predicted labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc74eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'observed': y_test, 'prediction': y_pred}\n",
    "labels_df = pd.DataFrame(labels)\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d5994",
   "metadata": {},
   "source": [
    "Evaluate the model using the `performance_metrics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, metrics = \"INSERT YOUR CODE HERE\"\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ad9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257c9c0e",
   "metadata": {},
   "source": [
    "How did it go? Recall is probably not that good, but it can be improved. Train the model again, but include the parameter `class_weight` and set it correctly. This addition tends to improve the performance of the model when we are dealing with unbalanced datasets. Do not forget to set the `penalty` parameter to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffca486",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model_balanced = \"INSERT YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e84590",
   "metadata": {},
   "source": [
    "Save the predictions in the variable called `y_pred_balanced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_balanced = \"INSERT YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94998c98",
   "metadata": {},
   "source": [
    "Once again, we will create a dataframe that contains the true and predicted labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_balanced = {'observed': y_test, 'prediction': y_pred_balanced}\n",
    "labels_balanced_df = pd.DataFrame(labels_balanced)\n",
    "labels_balanced_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12190c08",
   "metadata": {},
   "source": [
    "Evaluate the new model using the `performance_metrics` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490ea667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix, metrics = \"INSERT YOUR CODE HERE\"\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef97daa",
   "metadata": {},
   "source": [
    "You should see an improvement in the recall metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180aca2",
   "metadata": {},
   "source": [
    "## Data Imputation\n",
    "\n",
    "Data imputation is a statistical and data preprocessing technique used to fill in missing or incomplete values in a data set. When working with real-world data, it is common to encounter missing data points, which can result from a variety of reasons, such as data collection errors, sensor failures, or survey non-response. Data imputation aims to estimate or predict these missing values, making the data set more complete and suitable for analysis or modeling.\n",
    "\n",
    "Depending on the nature of the data and the specific problem, one or more data imputation methods are chosen. Some common imputation methods are\n",
    "\n",
    "- **Mean, Median, or Mode Imputation:** Replacing missing values with the mean (average), median, or mode of the observed values for that variable.\n",
    "- **Regression Imputation:** Using regression models to predict missing values based on other variables in the dataset.\n",
    "- **K-Nearest Neighbors (K-NN) Imputation:** Estimating missing values by considering the values of the nearest neighbors in the dataset.\n",
    "- **Interpolation:** Filling in missing values by estimating them based on adjacent data points in a time series or spatial dataset.\n",
    "- **Random Imputation:** Replacing missing values with randomly generated values, often drawn from the same distribution as the observed data.\n",
    "\n",
    "Data imputation is an essential step in preprocessing and preparing data for various data analysis and machine learning tasks. Imputed datasets allow you to retain more data for analysis and modeling, which can lead to better insights and more accurate predictions. However, it is important to be careful when applying data imputation and to choose the most appropriate method for the specific dataset and research problem, as improper imputation can lead to bias or inaccurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a27f22",
   "metadata": {},
   "source": [
    "### Mean Imputation\n",
    "\n",
    "As mentioned before, for some variables, the missing values of the dataset were replaced by zeros. Let us count how many zeros we have for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b0bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isin([0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f9b13",
   "metadata": {},
   "source": [
    "We can see that `SkinThickess` and `Insulin` are missing 215 and 359 values, respectively. We will deal with that later. For now, you will perform mean imputation for the other variables, however, keep in mind that imputation will make sense for some of these variables, so choose wisely. \n",
    "\n",
    "Mean imputation will be carried out in the following way: first, compute the means for each variable, but you will have to compute two in each case, one for the group of people who do not have diabetes, and the other one for the people who do have the disease; second, you will replace a missing value with the proper mean, for instance, if the missing value belongs to a person without diabetes and that corresponds to the variable `BMI`, then you will substitute the zero with the mean `BMI` of people who do not have the disease.\n",
    "\n",
    "Before going any further, let us create a copy of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51880d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b0bbed",
   "metadata": {},
   "source": [
    "Use the following cell to code your implementation of mean imputation, it should not take too many lines of code, by the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc71f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"INSERT YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4767c",
   "metadata": {},
   "source": [
    "You just wrote a few lines of code, didn't you? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b1898",
   "metadata": {},
   "source": [
    "### Regression Imputation\n",
    "\n",
    "You will carry out regression imputation to replace the missing values of the features `SkinThickess` and `Insulin`. In this case, you will perform this process twice, that is, you will perform regression imputation for each of these two variables.\n",
    "\n",
    "Let us begin with `SkinThickness`. First, you will have to create the data for training the linear regression model. In this case, the target variable is the non-zero values of `SkinThickness`. As predictors, you can use all the other variables, except for `Insulin`. Yes, I know, `Outcome` is a categorical variable and we did not talk about categorical variables in class, but in this case you do not have to do anything about this, so all good.  \n",
    "\n",
    "Use the following cell to create the training data of the first linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c07e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['SkinThickness', 'Insulin']\n",
    "X_linear_1 = \"INSERT YOUR CODE HERE\"\n",
    "y_linear_1 = \"INSERT YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca71a56",
   "metadata": {},
   "source": [
    "Train the first linear model in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f776aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_1 = \"INSERT YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68bb6b2",
   "metadata": {},
   "source": [
    "Create a test set with the observations for which the values of `SkinThickness` are equal to zero. You will use this set to obtain the predictions that will replace the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_linear_test_1 = \"INSERT YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35365e47",
   "metadata": {},
   "source": [
    "Create a new column called `Predictions` for the `X_linear_test_1` dataframe and store the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_linear_test['Predictions'] = \"INSERT YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceb88d6",
   "metadata": {},
   "source": [
    "Replace the missing values with your predictions in the `new_data` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.loc[\"INSERT YOUR CODE HERE\"] = X_linear_test['Predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea936f30",
   "metadata": {},
   "source": [
    "Note that we do not have zeros in the `SkinThickness` variable anymore, which is good news. Now repeat the same imputation for the `Insulin` variable in the following cell. By the way, keep the command `new_data.describe()` as the last line of code of the next cell.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_linear_2 = \"INSERT YOUR CODE HERE\"\n",
    "y_linear_2 = \"INSERT YOUR CODE HERE\"\n",
    "linear_model_2 = \"INSERT YOUR CODE HERE\"\n",
    "X_linear_test_2 = \"INSERT YOUR CODE HERE\"\n",
    "X_linear_test_2['Predictions'] = \"INSERT YOUR CODE HERE\"\n",
    "new_data.loc[new_data[\"INSERT YOUR CODE HERE\"] = X_linear_test_2['Predictions']\n",
    "new_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091ebf7a",
   "metadata": {},
   "source": [
    "As you can see, there must be some observations for which the predicted value of `Insulin` was a negative number. We should not allow that. Let us inspect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684850bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_data[new_data['Insulin'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b262a1",
   "metadata": {},
   "source": [
    "So it is just one observation, no big deal. We can replace the negative number with the mean of `Insulin`, but only considering the people who do not have diabetes. Do that in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe01172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"INSERT YOUR CODE HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a26d26",
   "metadata": {},
   "source": [
    "Sanity check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a34e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f37b6",
   "metadata": {},
   "source": [
    "All is well. Now we are ready for some logistic regression. Repeat the training of the model with the `new_data` dataframe. Remember to set the `penalty` parameter to `None` and include the proper setting of the `class_weight` parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97dc464",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = new_data['Outcome']\n",
    "X = new_data.drop(columns='Outcome')\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "logistic_model = \"INSERT YOUR CODE HERE\"\n",
    "y_pred_balanced = \"INSERT YOUR CODE HERE\"\n",
    "labels_balanced = {'observed': y_test, 'prediction': y_pred_balanced}\n",
    "labels_balanced_df = pd.DataFrame(labels_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd64c0e2",
   "metadata": {},
   "source": [
    "And now the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b51397",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, metrics = performance_metrics(labels_balanced_df)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c871206a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328cccf",
   "metadata": {},
   "source": [
    "Recall remained pretty much the same, but the other metrics were improved a tiny bit, so, was it worth it? If this were a Kaggle competition, absolutely!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
